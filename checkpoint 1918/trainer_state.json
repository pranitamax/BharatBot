{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1918,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010427528675703858,
      "grad_norm": 18.120403289794922,
      "learning_rate": 4.9843587069864446e-05,
      "loss": 6.9367,
      "step": 10
    },
    {
      "epoch": 0.020855057351407715,
      "grad_norm": 13.185827255249023,
      "learning_rate": 4.966979492526938e-05,
      "loss": 5.4588,
      "step": 20
    },
    {
      "epoch": 0.03128258602711158,
      "grad_norm": 2.1639935970306396,
      "learning_rate": 4.949600278067431e-05,
      "loss": 3.0298,
      "step": 30
    },
    {
      "epoch": 0.04171011470281543,
      "grad_norm": 1.119386911392212,
      "learning_rate": 4.932221063607925e-05,
      "loss": 1.4242,
      "step": 40
    },
    {
      "epoch": 0.05213764337851929,
      "grad_norm": 1.0928475856781006,
      "learning_rate": 4.914841849148419e-05,
      "loss": 1.1936,
      "step": 50
    },
    {
      "epoch": 0.06256517205422316,
      "grad_norm": 1.008676528930664,
      "learning_rate": 4.8974626346889125e-05,
      "loss": 1.1188,
      "step": 60
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 0.880106508731842,
      "learning_rate": 4.880083420229406e-05,
      "loss": 0.9594,
      "step": 70
    },
    {
      "epoch": 0.08342022940563086,
      "grad_norm": 1.399254560470581,
      "learning_rate": 4.8627042057699e-05,
      "loss": 1.0069,
      "step": 80
    },
    {
      "epoch": 0.09384775808133472,
      "grad_norm": 1.555087685585022,
      "learning_rate": 4.845324991310393e-05,
      "loss": 0.9677,
      "step": 90
    },
    {
      "epoch": 0.10427528675703858,
      "grad_norm": 1.214719295501709,
      "learning_rate": 4.8279457768508866e-05,
      "loss": 0.7741,
      "step": 100
    },
    {
      "epoch": 0.11470281543274244,
      "grad_norm": 0.7374926209449768,
      "learning_rate": 4.81056656239138e-05,
      "loss": 0.8755,
      "step": 110
    },
    {
      "epoch": 0.1251303441084463,
      "grad_norm": 1.0771262645721436,
      "learning_rate": 4.793187347931874e-05,
      "loss": 0.8505,
      "step": 120
    },
    {
      "epoch": 0.13555787278415016,
      "grad_norm": 1.1166532039642334,
      "learning_rate": 4.775808133472368e-05,
      "loss": 0.8039,
      "step": 130
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 1.2376327514648438,
      "learning_rate": 4.758428919012861e-05,
      "loss": 0.7916,
      "step": 140
    },
    {
      "epoch": 0.15641293013555788,
      "grad_norm": 1.4736762046813965,
      "learning_rate": 4.7410497045533545e-05,
      "loss": 0.9149,
      "step": 150
    },
    {
      "epoch": 0.16684045881126172,
      "grad_norm": 1.3194184303283691,
      "learning_rate": 4.7236704900938475e-05,
      "loss": 0.7221,
      "step": 160
    },
    {
      "epoch": 0.1772679874869656,
      "grad_norm": 1.5948114395141602,
      "learning_rate": 4.706291275634341e-05,
      "loss": 0.7285,
      "step": 170
    },
    {
      "epoch": 0.18769551616266944,
      "grad_norm": 1.3699121475219727,
      "learning_rate": 4.688912061174835e-05,
      "loss": 0.834,
      "step": 180
    },
    {
      "epoch": 0.1981230448383733,
      "grad_norm": 1.3540754318237305,
      "learning_rate": 4.6715328467153287e-05,
      "loss": 0.7266,
      "step": 190
    },
    {
      "epoch": 0.20855057351407716,
      "grad_norm": 1.4160616397857666,
      "learning_rate": 4.6541536322558224e-05,
      "loss": 0.8247,
      "step": 200
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 1.532230257987976,
      "learning_rate": 4.6367744177963154e-05,
      "loss": 0.7061,
      "step": 210
    },
    {
      "epoch": 0.22940563086548488,
      "grad_norm": 1.4779987335205078,
      "learning_rate": 4.619395203336809e-05,
      "loss": 0.753,
      "step": 220
    },
    {
      "epoch": 0.23983315954118875,
      "grad_norm": 1.7702499628067017,
      "learning_rate": 4.602015988877303e-05,
      "loss": 0.6056,
      "step": 230
    },
    {
      "epoch": 0.2502606882168926,
      "grad_norm": 1.3266823291778564,
      "learning_rate": 4.5846367744177965e-05,
      "loss": 0.8154,
      "step": 240
    },
    {
      "epoch": 0.26068821689259647,
      "grad_norm": 1.6202858686447144,
      "learning_rate": 4.56725755995829e-05,
      "loss": 0.7371,
      "step": 250
    },
    {
      "epoch": 0.2711157455683003,
      "grad_norm": 1.5946112871170044,
      "learning_rate": 4.549878345498784e-05,
      "loss": 0.7674,
      "step": 260
    },
    {
      "epoch": 0.28154327424400416,
      "grad_norm": 1.5447635650634766,
      "learning_rate": 4.532499131039277e-05,
      "loss": 0.668,
      "step": 270
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 1.6287524700164795,
      "learning_rate": 4.515119916579771e-05,
      "loss": 0.7005,
      "step": 280
    },
    {
      "epoch": 0.3023983315954119,
      "grad_norm": 2.4439802169799805,
      "learning_rate": 4.4977407021202644e-05,
      "loss": 0.6076,
      "step": 290
    },
    {
      "epoch": 0.31282586027111575,
      "grad_norm": 1.4160529375076294,
      "learning_rate": 4.480361487660758e-05,
      "loss": 0.6614,
      "step": 300
    },
    {
      "epoch": 0.3232533889468196,
      "grad_norm": 2.361733913421631,
      "learning_rate": 4.462982273201252e-05,
      "loss": 0.6277,
      "step": 310
    },
    {
      "epoch": 0.33368091762252344,
      "grad_norm": 2.1457204818725586,
      "learning_rate": 4.4456030587417455e-05,
      "loss": 0.6789,
      "step": 320
    },
    {
      "epoch": 0.34410844629822734,
      "grad_norm": 2.3549511432647705,
      "learning_rate": 4.4282238442822386e-05,
      "loss": 0.826,
      "step": 330
    },
    {
      "epoch": 0.3545359749739312,
      "grad_norm": 2.279325246810913,
      "learning_rate": 4.410844629822732e-05,
      "loss": 0.7287,
      "step": 340
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 1.6145037412643433,
      "learning_rate": 4.393465415363226e-05,
      "loss": 0.7733,
      "step": 350
    },
    {
      "epoch": 0.3753910323253389,
      "grad_norm": 1.5470696687698364,
      "learning_rate": 4.37608620090372e-05,
      "loss": 0.7344,
      "step": 360
    },
    {
      "epoch": 0.3858185610010427,
      "grad_norm": 1.99728262424469,
      "learning_rate": 4.3587069864442134e-05,
      "loss": 0.7883,
      "step": 370
    },
    {
      "epoch": 0.3962460896767466,
      "grad_norm": 1.309317946434021,
      "learning_rate": 4.3413277719847064e-05,
      "loss": 0.7502,
      "step": 380
    },
    {
      "epoch": 0.40667361835245047,
      "grad_norm": 2.4995901584625244,
      "learning_rate": 4.3239485575252e-05,
      "loss": 0.6401,
      "step": 390
    },
    {
      "epoch": 0.4171011470281543,
      "grad_norm": 1.6152366399765015,
      "learning_rate": 4.306569343065693e-05,
      "loss": 0.6515,
      "step": 400
    },
    {
      "epoch": 0.42752867570385816,
      "grad_norm": 1.6482816934585571,
      "learning_rate": 4.289190128606187e-05,
      "loss": 0.6162,
      "step": 410
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 1.6179380416870117,
      "learning_rate": 4.2718109141466806e-05,
      "loss": 0.6707,
      "step": 420
    },
    {
      "epoch": 0.4483837330552659,
      "grad_norm": 1.2844206094741821,
      "learning_rate": 4.254431699687174e-05,
      "loss": 0.5952,
      "step": 430
    },
    {
      "epoch": 0.45881126173096975,
      "grad_norm": 1.837235689163208,
      "learning_rate": 4.237052485227668e-05,
      "loss": 0.6296,
      "step": 440
    },
    {
      "epoch": 0.4692387904066736,
      "grad_norm": 1.9158202409744263,
      "learning_rate": 4.219673270768161e-05,
      "loss": 0.7156,
      "step": 450
    },
    {
      "epoch": 0.4796663190823775,
      "grad_norm": 1.5270395278930664,
      "learning_rate": 4.202294056308655e-05,
      "loss": 0.5922,
      "step": 460
    },
    {
      "epoch": 0.49009384775808135,
      "grad_norm": 2.091979742050171,
      "learning_rate": 4.1849148418491485e-05,
      "loss": 0.7146,
      "step": 470
    },
    {
      "epoch": 0.5005213764337852,
      "grad_norm": 1.6204347610473633,
      "learning_rate": 4.167535627389642e-05,
      "loss": 0.6333,
      "step": 480
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 1.8996353149414062,
      "learning_rate": 4.150156412930136e-05,
      "loss": 0.6021,
      "step": 490
    },
    {
      "epoch": 0.5213764337851929,
      "grad_norm": 2.2792181968688965,
      "learning_rate": 4.1327771984706296e-05,
      "loss": 0.7727,
      "step": 500
    },
    {
      "epoch": 0.5318039624608968,
      "grad_norm": 2.4268736839294434,
      "learning_rate": 4.1153979840111226e-05,
      "loss": 0.7741,
      "step": 510
    },
    {
      "epoch": 0.5422314911366006,
      "grad_norm": 1.8142977952957153,
      "learning_rate": 4.0980187695516163e-05,
      "loss": 0.8152,
      "step": 520
    },
    {
      "epoch": 0.5526590198123045,
      "grad_norm": 2.286170482635498,
      "learning_rate": 4.08063955509211e-05,
      "loss": 0.7848,
      "step": 530
    },
    {
      "epoch": 0.5630865484880083,
      "grad_norm": 1.6149840354919434,
      "learning_rate": 4.063260340632604e-05,
      "loss": 0.7694,
      "step": 540
    },
    {
      "epoch": 0.5735140771637122,
      "grad_norm": 2.1333160400390625,
      "learning_rate": 4.0458811261730975e-05,
      "loss": 0.5814,
      "step": 550
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 1.5957932472229004,
      "learning_rate": 4.0285019117135905e-05,
      "loss": 0.6961,
      "step": 560
    },
    {
      "epoch": 0.59436913451512,
      "grad_norm": 1.739457607269287,
      "learning_rate": 4.011122697254084e-05,
      "loss": 0.6778,
      "step": 570
    },
    {
      "epoch": 0.6047966631908238,
      "grad_norm": 1.9576972723007202,
      "learning_rate": 3.993743482794578e-05,
      "loss": 0.7168,
      "step": 580
    },
    {
      "epoch": 0.6152241918665277,
      "grad_norm": 1.5440746545791626,
      "learning_rate": 3.9763642683350716e-05,
      "loss": 0.7478,
      "step": 590
    },
    {
      "epoch": 0.6256517205422315,
      "grad_norm": 1.9990699291229248,
      "learning_rate": 3.9589850538755653e-05,
      "loss": 0.6726,
      "step": 600
    },
    {
      "epoch": 0.6360792492179353,
      "grad_norm": 2.3176069259643555,
      "learning_rate": 3.941605839416059e-05,
      "loss": 0.5592,
      "step": 610
    },
    {
      "epoch": 0.6465067778936392,
      "grad_norm": 2.27856183052063,
      "learning_rate": 3.924226624956552e-05,
      "loss": 0.691,
      "step": 620
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 1.9544490575790405,
      "learning_rate": 3.906847410497046e-05,
      "loss": 0.5854,
      "step": 630
    },
    {
      "epoch": 0.6673618352450469,
      "grad_norm": 1.8219610452651978,
      "learning_rate": 3.8894681960375395e-05,
      "loss": 0.6322,
      "step": 640
    },
    {
      "epoch": 0.6777893639207507,
      "grad_norm": 2.5687718391418457,
      "learning_rate": 3.872088981578033e-05,
      "loss": 0.6597,
      "step": 650
    },
    {
      "epoch": 0.6882168925964547,
      "grad_norm": 2.597356081008911,
      "learning_rate": 3.854709767118526e-05,
      "loss": 0.6444,
      "step": 660
    },
    {
      "epoch": 0.6986444212721585,
      "grad_norm": 1.6991078853607178,
      "learning_rate": 3.83733055265902e-05,
      "loss": 0.6178,
      "step": 670
    },
    {
      "epoch": 0.7090719499478624,
      "grad_norm": 1.9010342359542847,
      "learning_rate": 3.819951338199514e-05,
      "loss": 0.6532,
      "step": 680
    },
    {
      "epoch": 0.7194994786235662,
      "grad_norm": 3.0347845554351807,
      "learning_rate": 3.802572123740007e-05,
      "loss": 0.6615,
      "step": 690
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 1.7095868587493896,
      "learning_rate": 3.7851929092805004e-05,
      "loss": 0.6127,
      "step": 700
    },
    {
      "epoch": 0.7403545359749739,
      "grad_norm": 2.563230514526367,
      "learning_rate": 3.767813694820994e-05,
      "loss": 0.5068,
      "step": 710
    },
    {
      "epoch": 0.7507820646506778,
      "grad_norm": 2.150825262069702,
      "learning_rate": 3.750434480361488e-05,
      "loss": 0.5568,
      "step": 720
    },
    {
      "epoch": 0.7612095933263816,
      "grad_norm": 2.3586902618408203,
      "learning_rate": 3.7330552659019815e-05,
      "loss": 0.566,
      "step": 730
    },
    {
      "epoch": 0.7716371220020855,
      "grad_norm": 2.489555597305298,
      "learning_rate": 3.7156760514424746e-05,
      "loss": 0.7159,
      "step": 740
    },
    {
      "epoch": 0.7820646506777894,
      "grad_norm": 2.305001974105835,
      "learning_rate": 3.698296836982968e-05,
      "loss": 0.7726,
      "step": 750
    },
    {
      "epoch": 0.7924921793534933,
      "grad_norm": 2.4097068309783936,
      "learning_rate": 3.680917622523462e-05,
      "loss": 0.5504,
      "step": 760
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 2.50008487701416,
      "learning_rate": 3.663538408063956e-05,
      "loss": 0.6414,
      "step": 770
    },
    {
      "epoch": 0.8133472367049009,
      "grad_norm": 1.9620336294174194,
      "learning_rate": 3.6461591936044494e-05,
      "loss": 0.5488,
      "step": 780
    },
    {
      "epoch": 0.8237747653806048,
      "grad_norm": 3.263538360595703,
      "learning_rate": 3.628779979144943e-05,
      "loss": 0.5953,
      "step": 790
    },
    {
      "epoch": 0.8342022940563086,
      "grad_norm": 2.6053903102874756,
      "learning_rate": 3.611400764685436e-05,
      "loss": 0.6904,
      "step": 800
    },
    {
      "epoch": 0.8446298227320125,
      "grad_norm": 2.3954451084136963,
      "learning_rate": 3.59402155022593e-05,
      "loss": 0.6375,
      "step": 810
    },
    {
      "epoch": 0.8550573514077163,
      "grad_norm": 1.9933098554611206,
      "learning_rate": 3.5766423357664236e-05,
      "loss": 0.4394,
      "step": 820
    },
    {
      "epoch": 0.8654848800834203,
      "grad_norm": 1.9822872877120972,
      "learning_rate": 3.559263121306917e-05,
      "loss": 0.5694,
      "step": 830
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 2.8291189670562744,
      "learning_rate": 3.541883906847411e-05,
      "loss": 0.6451,
      "step": 840
    },
    {
      "epoch": 0.886339937434828,
      "grad_norm": 3.199366331100464,
      "learning_rate": 3.524504692387904e-05,
      "loss": 0.4706,
      "step": 850
    },
    {
      "epoch": 0.8967674661105318,
      "grad_norm": 2.673325538635254,
      "learning_rate": 3.507125477928398e-05,
      "loss": 0.5471,
      "step": 860
    },
    {
      "epoch": 0.9071949947862357,
      "grad_norm": 2.8150382041931152,
      "learning_rate": 3.4897462634688915e-05,
      "loss": 0.5395,
      "step": 870
    },
    {
      "epoch": 0.9176225234619395,
      "grad_norm": 1.8922226428985596,
      "learning_rate": 3.472367049009385e-05,
      "loss": 0.6254,
      "step": 880
    },
    {
      "epoch": 0.9280500521376434,
      "grad_norm": 2.5631978511810303,
      "learning_rate": 3.454987834549879e-05,
      "loss": 0.69,
      "step": 890
    },
    {
      "epoch": 0.9384775808133472,
      "grad_norm": 3.096997022628784,
      "learning_rate": 3.4376086200903726e-05,
      "loss": 0.658,
      "step": 900
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 4.125772953033447,
      "learning_rate": 3.4202294056308656e-05,
      "loss": 0.6668,
      "step": 910
    },
    {
      "epoch": 0.959332638164755,
      "grad_norm": 2.665250778198242,
      "learning_rate": 3.4028501911713587e-05,
      "loss": 0.5491,
      "step": 920
    },
    {
      "epoch": 0.9697601668404588,
      "grad_norm": 2.447812080383301,
      "learning_rate": 3.3854709767118524e-05,
      "loss": 0.6309,
      "step": 930
    },
    {
      "epoch": 0.9801876955161627,
      "grad_norm": 2.9861948490142822,
      "learning_rate": 3.368091762252346e-05,
      "loss": 0.4676,
      "step": 940
    },
    {
      "epoch": 0.9906152241918665,
      "grad_norm": 2.01373028755188,
      "learning_rate": 3.35071254779284e-05,
      "loss": 0.44,
      "step": 950
    },
    {
      "epoch": 1.0010427528675705,
      "grad_norm": 2.89844012260437,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.5595,
      "step": 960
    },
    {
      "epoch": 1.0114702815432743,
      "grad_norm": 2.9016966819763184,
      "learning_rate": 3.315954118873827e-05,
      "loss": 0.6169,
      "step": 970
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 2.2521026134490967,
      "learning_rate": 3.29857490441432e-05,
      "loss": 0.5759,
      "step": 980
    },
    {
      "epoch": 1.032325338894682,
      "grad_norm": 2.6568212509155273,
      "learning_rate": 3.281195689954814e-05,
      "loss": 0.6061,
      "step": 990
    },
    {
      "epoch": 1.0427528675703859,
      "grad_norm": 2.3849258422851562,
      "learning_rate": 3.2638164754953077e-05,
      "loss": 0.5839,
      "step": 1000
    },
    {
      "epoch": 1.0531803962460897,
      "grad_norm": 2.8946359157562256,
      "learning_rate": 3.2464372610358014e-05,
      "loss": 0.6458,
      "step": 1010
    },
    {
      "epoch": 1.0636079249217936,
      "grad_norm": 2.0224204063415527,
      "learning_rate": 3.229058046576295e-05,
      "loss": 0.5288,
      "step": 1020
    },
    {
      "epoch": 1.0740354535974974,
      "grad_norm": 2.8412554264068604,
      "learning_rate": 3.211678832116788e-05,
      "loss": 0.5788,
      "step": 1030
    },
    {
      "epoch": 1.0844629822732013,
      "grad_norm": 2.46798038482666,
      "learning_rate": 3.194299617657282e-05,
      "loss": 0.6284,
      "step": 1040
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 2.397243022918701,
      "learning_rate": 3.1769204031977755e-05,
      "loss": 0.5136,
      "step": 1050
    },
    {
      "epoch": 1.105318039624609,
      "grad_norm": 2.546158790588379,
      "learning_rate": 3.159541188738269e-05,
      "loss": 0.8104,
      "step": 1060
    },
    {
      "epoch": 1.1157455683003128,
      "grad_norm": 2.6357791423797607,
      "learning_rate": 3.142161974278763e-05,
      "loss": 0.6439,
      "step": 1070
    },
    {
      "epoch": 1.1261730969760166,
      "grad_norm": 2.4457898139953613,
      "learning_rate": 3.1247827598192567e-05,
      "loss": 0.4526,
      "step": 1080
    },
    {
      "epoch": 1.1366006256517205,
      "grad_norm": 3.286105155944824,
      "learning_rate": 3.10740354535975e-05,
      "loss": 0.5158,
      "step": 1090
    },
    {
      "epoch": 1.1470281543274243,
      "grad_norm": 3.3191640377044678,
      "learning_rate": 3.0900243309002434e-05,
      "loss": 0.5377,
      "step": 1100
    },
    {
      "epoch": 1.1574556830031282,
      "grad_norm": 2.387174129486084,
      "learning_rate": 3.072645116440737e-05,
      "loss": 0.5511,
      "step": 1110
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 2.7200353145599365,
      "learning_rate": 3.055265901981231e-05,
      "loss": 0.5388,
      "step": 1120
    },
    {
      "epoch": 1.178310740354536,
      "grad_norm": 2.1796228885650635,
      "learning_rate": 3.0378866875217242e-05,
      "loss": 0.5263,
      "step": 1130
    },
    {
      "epoch": 1.1887382690302397,
      "grad_norm": 2.909252882003784,
      "learning_rate": 3.020507473062218e-05,
      "loss": 0.563,
      "step": 1140
    },
    {
      "epoch": 1.1991657977059438,
      "grad_norm": 2.7469825744628906,
      "learning_rate": 3.0031282586027116e-05,
      "loss": 0.6881,
      "step": 1150
    },
    {
      "epoch": 1.2095933263816476,
      "grad_norm": 2.4347753524780273,
      "learning_rate": 2.985749044143205e-05,
      "loss": 0.4767,
      "step": 1160
    },
    {
      "epoch": 1.2200208550573515,
      "grad_norm": 2.40594220161438,
      "learning_rate": 2.9683698296836987e-05,
      "loss": 0.5682,
      "step": 1170
    },
    {
      "epoch": 1.2304483837330553,
      "grad_norm": 2.4845738410949707,
      "learning_rate": 2.9509906152241917e-05,
      "loss": 0.6226,
      "step": 1180
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 3.770871162414551,
      "learning_rate": 2.9336114007646854e-05,
      "loss": 0.5232,
      "step": 1190
    },
    {
      "epoch": 1.251303441084463,
      "grad_norm": 2.7885475158691406,
      "learning_rate": 2.9162321863051788e-05,
      "loss": 0.516,
      "step": 1200
    },
    {
      "epoch": 1.2617309697601669,
      "grad_norm": 2.8997738361358643,
      "learning_rate": 2.8988529718456725e-05,
      "loss": 0.6163,
      "step": 1210
    },
    {
      "epoch": 1.2721584984358707,
      "grad_norm": 2.302232503890991,
      "learning_rate": 2.8814737573861662e-05,
      "loss": 0.6302,
      "step": 1220
    },
    {
      "epoch": 1.2825860271115745,
      "grad_norm": 2.128685235977173,
      "learning_rate": 2.8640945429266596e-05,
      "loss": 0.5374,
      "step": 1230
    },
    {
      "epoch": 1.2930135557872784,
      "grad_norm": 2.511390209197998,
      "learning_rate": 2.8467153284671533e-05,
      "loss": 0.6978,
      "step": 1240
    },
    {
      "epoch": 1.3034410844629822,
      "grad_norm": 4.021928310394287,
      "learning_rate": 2.829336114007647e-05,
      "loss": 0.5074,
      "step": 1250
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 2.7664875984191895,
      "learning_rate": 2.8119568995481404e-05,
      "loss": 0.4486,
      "step": 1260
    },
    {
      "epoch": 1.32429614181439,
      "grad_norm": 2.3021836280822754,
      "learning_rate": 2.794577685088634e-05,
      "loss": 0.6254,
      "step": 1270
    },
    {
      "epoch": 1.3347236704900938,
      "grad_norm": 3.5964019298553467,
      "learning_rate": 2.7771984706291275e-05,
      "loss": 0.5137,
      "step": 1280
    },
    {
      "epoch": 1.3451511991657976,
      "grad_norm": 3.021564245223999,
      "learning_rate": 2.7598192561696212e-05,
      "loss": 0.5578,
      "step": 1290
    },
    {
      "epoch": 1.3555787278415017,
      "grad_norm": 3.21832013130188,
      "learning_rate": 2.742440041710115e-05,
      "loss": 0.6468,
      "step": 1300
    },
    {
      "epoch": 1.3660062565172053,
      "grad_norm": 2.9344727993011475,
      "learning_rate": 2.7250608272506083e-05,
      "loss": 0.5616,
      "step": 1310
    },
    {
      "epoch": 1.3764337851929094,
      "grad_norm": 4.268043041229248,
      "learning_rate": 2.707681612791102e-05,
      "loss": 0.8093,
      "step": 1320
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 3.3525474071502686,
      "learning_rate": 2.6903023983315957e-05,
      "loss": 0.488,
      "step": 1330
    },
    {
      "epoch": 1.397288842544317,
      "grad_norm": 2.9594876766204834,
      "learning_rate": 2.672923183872089e-05,
      "loss": 0.4642,
      "step": 1340
    },
    {
      "epoch": 1.407716371220021,
      "grad_norm": 3.481078863143921,
      "learning_rate": 2.6555439694125828e-05,
      "loss": 0.6041,
      "step": 1350
    },
    {
      "epoch": 1.4181438998957248,
      "grad_norm": 3.4002089500427246,
      "learning_rate": 2.6381647549530765e-05,
      "loss": 0.448,
      "step": 1360
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 3.8882453441619873,
      "learning_rate": 2.62078554049357e-05,
      "loss": 0.5834,
      "step": 1370
    },
    {
      "epoch": 1.4389989572471324,
      "grad_norm": 2.788372039794922,
      "learning_rate": 2.6034063260340636e-05,
      "loss": 0.6458,
      "step": 1380
    },
    {
      "epoch": 1.4494264859228363,
      "grad_norm": 3.2053442001342773,
      "learning_rate": 2.5860271115745573e-05,
      "loss": 0.5205,
      "step": 1390
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 3.6905956268310547,
      "learning_rate": 2.5686478971150506e-05,
      "loss": 0.5289,
      "step": 1400
    },
    {
      "epoch": 1.470281543274244,
      "grad_norm": 3.043086051940918,
      "learning_rate": 2.5512686826555444e-05,
      "loss": 0.6052,
      "step": 1410
    },
    {
      "epoch": 1.4807090719499478,
      "grad_norm": 2.6895463466644287,
      "learning_rate": 2.533889468196038e-05,
      "loss": 0.6055,
      "step": 1420
    },
    {
      "epoch": 1.4911366006256517,
      "grad_norm": 4.439086437225342,
      "learning_rate": 2.5165102537365314e-05,
      "loss": 0.5412,
      "step": 1430
    },
    {
      "epoch": 1.5015641293013555,
      "grad_norm": 2.8890938758850098,
      "learning_rate": 2.4991310392770248e-05,
      "loss": 0.5617,
      "step": 1440
    },
    {
      "epoch": 1.5119916579770596,
      "grad_norm": 3.4580721855163574,
      "learning_rate": 2.4817518248175185e-05,
      "loss": 0.5532,
      "step": 1450
    },
    {
      "epoch": 1.5224191866527632,
      "grad_norm": 2.766920566558838,
      "learning_rate": 2.464372610358012e-05,
      "loss": 0.4732,
      "step": 1460
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 4.00832986831665,
      "learning_rate": 2.4469933958985056e-05,
      "loss": 0.5894,
      "step": 1470
    },
    {
      "epoch": 1.543274244004171,
      "grad_norm": 3.3603665828704834,
      "learning_rate": 2.4296141814389993e-05,
      "loss": 0.5068,
      "step": 1480
    },
    {
      "epoch": 1.553701772679875,
      "grad_norm": 2.816767930984497,
      "learning_rate": 2.4122349669794927e-05,
      "loss": 0.5757,
      "step": 1490
    },
    {
      "epoch": 1.5641293013555786,
      "grad_norm": 3.005924940109253,
      "learning_rate": 2.3948557525199864e-05,
      "loss": 0.6005,
      "step": 1500
    },
    {
      "epoch": 1.5745568300312827,
      "grad_norm": 2.77915358543396,
      "learning_rate": 2.3774765380604798e-05,
      "loss": 0.4952,
      "step": 1510
    },
    {
      "epoch": 1.5849843587069863,
      "grad_norm": 3.5553653240203857,
      "learning_rate": 2.360097323600973e-05,
      "loss": 0.576,
      "step": 1520
    },
    {
      "epoch": 1.5954118873826904,
      "grad_norm": 3.62424635887146,
      "learning_rate": 2.342718109141467e-05,
      "loss": 0.5539,
      "step": 1530
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 3.7791571617126465,
      "learning_rate": 2.3253388946819606e-05,
      "loss": 0.6864,
      "step": 1540
    },
    {
      "epoch": 1.616266944734098,
      "grad_norm": 3.48371958732605,
      "learning_rate": 2.307959680222454e-05,
      "loss": 0.5297,
      "step": 1550
    },
    {
      "epoch": 1.6266944734098019,
      "grad_norm": 3.0352537631988525,
      "learning_rate": 2.2905804657629476e-05,
      "loss": 0.4799,
      "step": 1560
    },
    {
      "epoch": 1.6371220020855057,
      "grad_norm": 3.1687915325164795,
      "learning_rate": 2.2732012513034413e-05,
      "loss": 0.4693,
      "step": 1570
    },
    {
      "epoch": 1.6475495307612096,
      "grad_norm": 3.4791040420532227,
      "learning_rate": 2.2558220368439347e-05,
      "loss": 0.5685,
      "step": 1580
    },
    {
      "epoch": 1.6579770594369134,
      "grad_norm": 4.900234699249268,
      "learning_rate": 2.2384428223844284e-05,
      "loss": 0.3389,
      "step": 1590
    },
    {
      "epoch": 1.6684045881126173,
      "grad_norm": 3.5834295749664307,
      "learning_rate": 2.2210636079249218e-05,
      "loss": 0.5474,
      "step": 1600
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 2.9589920043945312,
      "learning_rate": 2.2036843934654155e-05,
      "loss": 0.5496,
      "step": 1610
    },
    {
      "epoch": 1.6892596454640252,
      "grad_norm": 3.315258026123047,
      "learning_rate": 2.1863051790059092e-05,
      "loss": 0.5223,
      "step": 1620
    },
    {
      "epoch": 1.6996871741397288,
      "grad_norm": 3.0631203651428223,
      "learning_rate": 2.1689259645464026e-05,
      "loss": 0.5114,
      "step": 1630
    },
    {
      "epoch": 1.7101147028154329,
      "grad_norm": 3.49934983253479,
      "learning_rate": 2.151546750086896e-05,
      "loss": 0.4942,
      "step": 1640
    },
    {
      "epoch": 1.7205422314911365,
      "grad_norm": 3.8816208839416504,
      "learning_rate": 2.1341675356273897e-05,
      "loss": 0.5584,
      "step": 1650
    },
    {
      "epoch": 1.7309697601668406,
      "grad_norm": 2.18363356590271,
      "learning_rate": 2.1167883211678834e-05,
      "loss": 0.4615,
      "step": 1660
    },
    {
      "epoch": 1.7413972888425442,
      "grad_norm": 2.881683111190796,
      "learning_rate": 2.0994091067083768e-05,
      "loss": 0.4264,
      "step": 1670
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 3.5457630157470703,
      "learning_rate": 2.0820298922488705e-05,
      "loss": 0.4075,
      "step": 1680
    },
    {
      "epoch": 1.7622523461939519,
      "grad_norm": 4.673979759216309,
      "learning_rate": 2.064650677789364e-05,
      "loss": 0.4921,
      "step": 1690
    },
    {
      "epoch": 1.772679874869656,
      "grad_norm": 3.310748338699341,
      "learning_rate": 2.0472714633298575e-05,
      "loss": 0.4247,
      "step": 1700
    },
    {
      "epoch": 1.7831074035453598,
      "grad_norm": 4.127343654632568,
      "learning_rate": 2.0298922488703513e-05,
      "loss": 0.6099,
      "step": 1710
    },
    {
      "epoch": 1.7935349322210636,
      "grad_norm": 4.071564197540283,
      "learning_rate": 2.0125130344108446e-05,
      "loss": 0.424,
      "step": 1720
    },
    {
      "epoch": 1.8039624608967675,
      "grad_norm": 3.012152671813965,
      "learning_rate": 1.9951338199513383e-05,
      "loss": 0.5734,
      "step": 1730
    },
    {
      "epoch": 1.8143899895724713,
      "grad_norm": 3.1309304237365723,
      "learning_rate": 1.977754605491832e-05,
      "loss": 0.4821,
      "step": 1740
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 3.9718832969665527,
      "learning_rate": 1.9603753910323254e-05,
      "loss": 0.4269,
      "step": 1750
    },
    {
      "epoch": 1.835245046923879,
      "grad_norm": 3.05123233795166,
      "learning_rate": 1.942996176572819e-05,
      "loss": 0.4792,
      "step": 1760
    },
    {
      "epoch": 1.8456725755995829,
      "grad_norm": 3.3670177459716797,
      "learning_rate": 1.9256169621133125e-05,
      "loss": 0.5446,
      "step": 1770
    },
    {
      "epoch": 1.8561001042752867,
      "grad_norm": 3.7042253017425537,
      "learning_rate": 1.908237747653806e-05,
      "loss": 0.4661,
      "step": 1780
    },
    {
      "epoch": 1.8665276329509908,
      "grad_norm": 3.977311134338379,
      "learning_rate": 1.8908585331942996e-05,
      "loss": 0.5509,
      "step": 1790
    },
    {
      "epoch": 1.8769551616266944,
      "grad_norm": 2.3433282375335693,
      "learning_rate": 1.8734793187347933e-05,
      "loss": 0.5795,
      "step": 1800
    },
    {
      "epoch": 1.8873826903023985,
      "grad_norm": 4.254759788513184,
      "learning_rate": 1.8561001042752867e-05,
      "loss": 0.4528,
      "step": 1810
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 4.578836917877197,
      "learning_rate": 1.8387208898157804e-05,
      "loss": 0.5057,
      "step": 1820
    },
    {
      "epoch": 1.9082377476538062,
      "grad_norm": 3.2877955436706543,
      "learning_rate": 1.821341675356274e-05,
      "loss": 0.5922,
      "step": 1830
    },
    {
      "epoch": 1.9186652763295098,
      "grad_norm": 3.562495470046997,
      "learning_rate": 1.8039624608967675e-05,
      "loss": 0.5243,
      "step": 1840
    },
    {
      "epoch": 1.9290928050052139,
      "grad_norm": 4.229121685028076,
      "learning_rate": 1.786583246437261e-05,
      "loss": 0.3755,
      "step": 1850
    },
    {
      "epoch": 1.9395203336809175,
      "grad_norm": 3.3263118267059326,
      "learning_rate": 1.769204031977755e-05,
      "loss": 0.4898,
      "step": 1860
    },
    {
      "epoch": 1.9499478623566215,
      "grad_norm": 4.719244480133057,
      "learning_rate": 1.7518248175182482e-05,
      "loss": 0.5039,
      "step": 1870
    },
    {
      "epoch": 1.9603753910323254,
      "grad_norm": 2.692997932434082,
      "learning_rate": 1.734445603058742e-05,
      "loss": 0.5521,
      "step": 1880
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 2.859930992126465,
      "learning_rate": 1.7170663885992357e-05,
      "loss": 0.5467,
      "step": 1890
    },
    {
      "epoch": 1.981230448383733,
      "grad_norm": 3.4834177494049072,
      "learning_rate": 1.6996871741397287e-05,
      "loss": 0.4649,
      "step": 1900
    },
    {
      "epoch": 1.991657977059437,
      "grad_norm": 3.001912832260132,
      "learning_rate": 1.6823079596802224e-05,
      "loss": 0.5584,
      "step": 1910
    }
  ],
  "logging_steps": 10,
  "max_steps": 2877,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8310922643767296.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
