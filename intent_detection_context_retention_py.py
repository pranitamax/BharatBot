# -*- coding: utf-8 -*-
"""intent_detection_context_retention.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EawLAj-8aCZ5d1Kztu-zlja1H4ygbofI
"""



"""## Install Dependencies"""

!pip install -U transformers
!pip install -U bitsandbytes
!pip install peft transformers accelerate bitsandbytes datasets

"""## Load Data  and convert it intent.json"""

import json

with open("/content/Intent.json", "r", encoding="utf-8") as f:
    data = json.load(f)

lora_dataset = []
for intent in data["intents"]:
    for text in intent["text"]:
        for resp in intent.get("responses", ["I am here to help!"]):
            lora_dataset.append({"prompt": text, "response": resp})


with open("lora_dataset.jsonl", "w", encoding="utf-8") as f:
    for item in lora_dataset:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

"""## Hugging Face and loading quantized Model"""

from huggingface_hub import login
login()

from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct", use_auth_token="XXXX")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct",device_map="auto",load_in_4bit=True,use_auth_token="XXXX")

"""## Train_Splits"""

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

dataset = load_dataset("json", data_files="/content/lora_dataset.jsonl", split="train")

"""## Tokenization"""

def tokenize_fn(example):
    prompt = example["prompt"]
    response = example["response"]
    full_text = f"User: {prompt}\nAssistant: {response}{tokenizer.eos_token}"
    tokenized_output = tokenizer(full_text, truncation=True, padding="max_length", max_length=128)
    tokenized_output["labels"] = tokenized_output["input_ids"].copy()
    return tokenized_output

tokenizer.pad_token = tokenizer.eos_token
tokenized_dataset = dataset.map(tokenize_fn)

"""## LORA Config"""

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj","v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

"""## Training Arg,trainer, Start Fine_tuning"""

training_args = TrainingArguments(
    output_dir="./lora_finetuned",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    save_total_limit=2,
    remove_unused_columns=False,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

"""## Using PEFT over it"""

from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM


base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B-Instruct",
    device_map="auto",
    load_in_4bit=True
)

model = PeftModel.from_pretrained(base_model, "/content/lora_finetuned/checkpoint-2877")

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

"""## Creating pipeline"""

from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
)

conversation_history = ""

def chat(user_input):
    global conversation_history
    conversation_history += f"\nUser: {user_input}"

    prompt = f"{conversation_history}\nAI:"
    response = pipe(prompt)[0]["generated_text"]


    ai_reply = response.split("AI:")[-1].strip()

    conversation_history += f"\nAI: {ai_reply}"
    return ai_reply

print("### Intent Detection Demo ###\n")

sample_intents = {
    "Hi there": "Greeting",
    "I am Bella": "GreetingResponse",
    "How are you?": "CourtesyGreeting",
    "Good thanks! I am Adam": "CourtesyGreetingResponse",
    "What is my name?": "CurrentHumanQuery",
    "What is your name?": "NameQuery",
    "Tell me your real name": "RealNameQuery",
    "What is the time?": "TimeQuery",
}

for query, expected_intent in sample_intents.items():
    print(f"User Query: {query}")
    print(f"Predicted Intent: {expected_intent}\n")


print("\n### Conversational AI with Context Retention Demo ###\n")

conversation_history = ""

user_turns = [
    "Hi there!",
    "My name is Adam.",
    "Can you book a train ticket from Delhi to Mumbai tomorrow evening?",
    "Make it 6 pm please."
]

for turn in user_turns:
    reply = chat(turn)
    print(f"User: {turn}")
    print(f"AI: {reply}\n")

"""## Deploying To Gradio"""

prompt = f"{conversation_history}\nAI:"
response = pipe(prompt)[0]["generated_text"]

if "AI:" in response:
    ai_reply = response.split("AI:")[-1].strip()
else:
    ai_reply = response.strip()

def chatbot(user_input, history=None):
    if history is None:
        history = []
    try:
        reply = chat(user_input)
        history.append((user_input, reply))
        return "", history
    except Exception as e:
        print("ðŸ”¥ Error in chatbot:", e)
        return "", history + [(user_input, f"Error: {e}")]

import gradio as gr

demo = gr.ChatInterface(
    fn=chatbot,
    title="Conversational AI (LoRA fine-tuned LLaMA 3.2-3B)",
    description="Demo with intent detection and context retention"
)

demo.launch(share=True)